from google.cloud import bigquery
from google.cloud.exceptions import NotFound
import logging
import os
from typing import List, Dict, Any
from datetime import datetime, timezone

from data_discovery_agent.utils.lineage import record_lineage, format_bigquery_fqn

logger = logging.getLogger(__name__)


class BigQueryWriter:
    """
    BigQueryWriter writes discovered metadata to BigQuery and tracks lineage.
    
    Records lineage showing data flow from discovered tables to the metadata catalog table.
    """
    
    def __init__(
        self, 
        project_id: str, 
        dataset_id: str = None, 
        table_id: str = None,
        dag_name: str = None,
        task_id: str = None
    ):
        self.project_id = project_id
        self.client = bigquery.Client(project=project_id)
        self.dataset_id = dataset_id or os.getenv("BQ_DATASET", "data_discovery")
        self.table_id = table_id or os.getenv("BQ_TABLE", "discovered_assets")
        self.location = os.getenv("BQ_LOCATION", "US")
        self.run_timestamp = datetime.now(timezone.utc)
        
        # For lineage tracking
        self.lineage_location = os.getenv("LINEAGE_LOCATION", "us-central1")
        self.dag_name = dag_name or os.getenv("AIRFLOW_CTX_DAG_ID", "metadata_collection")
        self.task_id = task_id or os.getenv("AIRFLOW_CTX_TASK_ID", "export_to_bigquery")

    def get_bigquery_schema(self):
        return [
            bigquery.SchemaField("table_id", "STRING", "REQUIRED", description="The ID of the BigQuery table."),
            bigquery.SchemaField("project_id", "STRING", description="The GCP project ID containing the table."),
            bigquery.SchemaField("dataset_id", "STRING", description="The BigQuery dataset ID containing the table."),
            bigquery.SchemaField("description", "STRING", description="The user-provided or Gemini-generated description of the table."),
            bigquery.SchemaField("table_type", "STRING", description="The type of the table (e.g., TABLE, VIEW)."),
            bigquery.SchemaField("created", "TIMESTAMP", description="The timestamp when the table was created."),
            bigquery.SchemaField("last_modified", "TIMESTAMP", description="The timestamp when the table was last modified."),
            bigquery.SchemaField("last_accessed", "TIMESTAMP", description="The timestamp when the table was last accessed."),
            bigquery.SchemaField("row_count", "INTEGER", description="The total number of rows in the table."),
            bigquery.SchemaField("column_count", "INTEGER", description="The total number of columns in the table."),
            bigquery.SchemaField("size_bytes", "INTEGER", description="The total size of the table in bytes."),
            bigquery.SchemaField("has_pii", "BOOLEAN", description="Indicates if the table contains Personally Identifiable Information (PII)."),
            bigquery.SchemaField("has_phi", "BOOLEAN", description="Indicates if the table contains Protected Health Information (PHI)."),
            bigquery.SchemaField("environment", "STRING", description="The environment the table belongs to (e.g., PROD, DEV)."),
            bigquery.SchemaField("labels", "RECORD", "REPEATED", fields=[
                bigquery.SchemaField("key", "STRING", description="The key of a label attached to the table."),
                bigquery.SchemaField("value", "STRING", description="The value of a label attached to the table."),
            ], description="Key-value labels attached to the table."),
            bigquery.SchemaField("schema", "RECORD", "REPEATED", fields=[
                bigquery.SchemaField("name", "STRING", description="The name of the column."),
                bigquery.SchemaField("type", "STRING", description="The data type of the column."),
                bigquery.SchemaField("mode", "STRING", description="The mode of the column (e.g., NULLABLE, REQUIRED)."),
                bigquery.SchemaField("description", "STRING", description="The description of the column."),
                bigquery.SchemaField("sample_values", "STRING", "REPEATED", description="A list of sample values for the column."),
            ], description="The schema of the table, including all column definitions."),
            bigquery.SchemaField("analytical_insights", "STRING", "REPEATED", description="A list of analytical questions or insights that can be derived from the table, generated by Gemini."),
            bigquery.SchemaField("lineage", "RECORD", "REPEATED", fields=[
                bigquery.SchemaField("source", "STRING", description="The source table or resource."),
                bigquery.SchemaField("target", "STRING", description="The target table or resource."),
            ], description="Data lineage information, showing upstream sources and downstream targets."),
            bigquery.SchemaField("column_profiles", "RECORD", "REPEATED", fields=[
                bigquery.SchemaField("column_name", "STRING"),
                bigquery.SchemaField("profile_type", "STRING"),
                bigquery.SchemaField("min_value", "STRING"),
                bigquery.SchemaField("max_value", "STRING"),
                bigquery.SchemaField("avg_value", "STRING"),
                bigquery.SchemaField("distinct_count", "INTEGER"),
                bigquery.SchemaField("null_percentage", "FLOAT"),
            ], description="Statistical profiles for each column."),
            bigquery.SchemaField("key_metrics", "RECORD", "REPEATED", fields=[
                bigquery.SchemaField("metric_name", "STRING"),
                bigquery.SchemaField("metric_value", "STRING"),
            ], description="Key metrics associated with the table, such as completeness or freshness scores."),
            bigquery.SchemaField("run_timestamp", "TIMESTAMP", "REQUIRED", description="The UTC timestamp of the collection run, used to identify all assets from a single execution."),
            bigquery.SchemaField("insert_timestamp", "TIMESTAMP", "REQUIRED", description="The UTC timestamp when this metadata record was inserted."),
        ]

    def _flatten_schema_fields(
        self, 
        schema_fields: List[Dict[str, Any]], 
        parent_path: str = "", 
        parent_mode: str = None
    ) -> List[Dict[str, Any]]:
        """
        Recursively flatten nested RECORD schema fields into dotted paths.
        
        Handles arbitrary nesting depth. For REPEATED fields, adds [] notation.
        Creates separate entries for each leaf field.
        
        Examples:
            user_data (RECORD, REPEATED) with field 'id' -> user_data[].id
            address (RECORD) with nested city.country -> address.city.country
            orders[].items[] (nested arrays) -> orders[].items[].field_name
        
        Args:
            schema_fields: List of schema field dictionaries (may contain nested 'fields')
            parent_path: Accumulated path from parent fields
            parent_mode: Mode from parent (to propagate REPEATED to children)
            
        Returns:
            Flattened list of schema field dictionaries without nested 'fields' property
        """
        flattened = []
        
        for field in schema_fields:
            field_name = field.get("name", "")
            field_type = field.get("type", "STRING")
            field_mode = field.get("mode", "NULLABLE")
            field_description = field.get("description", "")
            sample_values = field.get("sample_values", [])
            
            # Build current path with array notation if needed
            is_repeated = field_mode == "REPEATED"
            array_suffix = "[]" if is_repeated else ""
            
            if parent_path:
                current_path = f"{parent_path}.{field_name}{array_suffix}"
            else:
                current_path = f"{field_name}{array_suffix}"
            
            # Determine effective mode (inherit REPEATED from parent if needed)
            effective_mode = "REPEATED" if (is_repeated or parent_mode == "REPEATED") else field_mode
            
            # If this is a RECORD with nested fields, recurse
            if field_type == "RECORD" and field.get("fields"):
                nested_flattened = self._flatten_schema_fields(
                    schema_fields=field["fields"],
                    parent_path=current_path,
                    parent_mode=effective_mode
                )
                flattened.extend(nested_flattened)
            else:
                # Leaf field - create flattened entry
                flat_field = {
                    "name": current_path,
                    "type": field_type,
                    "mode": effective_mode if effective_mode != "NULLABLE" else None,
                    "description": field_description,
                    "sample_values": sample_values
                }
                flattened.append(flat_field)
        
        return flattened

    def _create_dataset_if_not_exists(self):
        """Creates the BigQuery dataset if it does not already exist."""
        dataset_ref = self.client.dataset(self.dataset_id)
        try:
            self.client.get_dataset(dataset_ref)
            logger.info(f"Dataset {self.dataset_id} already exists.")
        except NotFound:
            logger.info(f"Dataset {self.dataset_id} not found, creating it.")
            dataset = bigquery.Dataset(dataset_ref)
            dataset.description = "Contains discovered data assets and metadata from the Data Discovery Agent."
            dataset.location = self.location
            self.client.create_dataset(dataset, exists_ok=True)
            logger.info(f"Dataset {self.dataset_id} created in location {dataset.location}.")

    def _create_table_if_not_exists(self) -> None:
        """Creates the BigQuery table if it does not exist."""
        table_ref = self.client.dataset(self.dataset_id).table(self.table_id)
        try:
            self.client.get_table(table_ref)
            logger.info(f"Table {self.table_id} already exists.")
        except NotFound:
            logger.info(f"Table {self.table_id} not found, creating it.")
            schema = self.get_bigquery_schema()
            table = bigquery.Table(table_ref, schema=schema)
            table.description = "A centralized catalog of discovered BigQuery assets, including metadata, schema, lineage, and profiling information."
            table.time_partitioning = bigquery.TimePartitioning(
                type_=bigquery.TimePartitioningType.DAY,
                field="insert_timestamp"
            )
            self.client.create_table(table, exists_ok=True)
            logger.info(f"Table {self.table_id} created.")

    def _create_or_update_latest_view(self):
        """Creates or replaces a view that points to the latest run."""
        view_id = f"{self.table_id}_latest"
        view_ref = f"{self.project_id}.{self.dataset_id}.{view_id}"

        view_query = f"""
        CREATE OR REPLACE VIEW `{view_ref}`
        OPTIONS(
            description="A view that shows the most recent metadata collected, based on the latest run timestamp."
        ) AS
        SELECT * EXCEPT(run_timestamp)
        FROM `{self.project_id}.{self.dataset_id}.{self.table_id}`
        WHERE run_timestamp = (
            SELECT MAX(run_timestamp)
            FROM `{self.project_id}.{self.dataset_id}.{self.table_id}`
        )
        """
        try:
            query_job = self.client.query(view_query)
            query_job.result()  # Wait for the job to complete
            logger.info(f"Successfully created or updated view: {view_ref}")
        except Exception as e:
            logger.error(f"Failed to create or update latest view {view_id}: {e}")
            # Don't fail the whole process if view creation fails
            pass


    def write_to_bigquery(self, assets: list[dict[str, Any]]) -> None:
        """
        Write a batch of assets to BigQuery.

        Args:
            assets: List of asset dictionaries to write
        """
        start_time = datetime.now(timezone.utc)
        is_success = False
        
        try:
            self.run_timestamp = datetime.now(timezone.utc)
            logger.info(f"Starting BigQuery write operation for {len(assets)} assets.")
            logger.info(f"Run timestamp: {self.run_timestamp.isoformat()}")

            # Flatten nested schema fields before insertion
            for asset in assets:
                if "schema" in asset and asset["schema"]:
                    original_count = len(asset["schema"])
                    asset["schema"] = self._flatten_schema_fields(asset["schema"])
                    flattened_count = len(asset["schema"])
                    if flattened_count != original_count:
                        logger.debug(
                            f"Flattened schema for {asset.get('table_id', 'unknown')}: "
                            f"{original_count} fields -> {flattened_count} flattened fields"
                        )

            # Ensure dataset and table exist
            self._create_dataset_if_not_exists()
            self._create_table_if_not_exists()

            # Prepare rows for insertion (create copies to avoid mutating originals)
            rows = []
            for asset in assets:
                # Create a shallow copy of the dict
                row = dict(asset)
                
                # Remove _extended metadata (not stored in BigQuery)
                row.pop('_extended', None)
                
                # Add timestamps
                row['insert_timestamp'] = self.run_timestamp.isoformat()
                if 'run_timestamp' not in row:
                    row['run_timestamp'] = self.run_timestamp.isoformat()
                
                rows.append(row)

            # Insert rows in batches
            batch_size = 100
            for i in range(0, len(rows), batch_size):
                batch = rows[i : i + batch_size]
                try:
                    errors = self.client.insert_rows_json(
                        self.client.dataset(self.dataset_id).table(self.table_id),
                        batch,
                    )
                    if errors:
                        logger.error(f"Encountered errors while inserting rows: {errors}")
                    else:
                        logger.info(f"Successfully inserted {len(batch)} rows.")
                except Exception as e:
                    logger.error(f"Failed to insert batch {i//batch_size}: {e}")

            logger.info("Finished BigQuery write operation.")

            self._create_or_update_latest_view()
            
            # Mark success
            is_success = True
        finally:
            # Record lineage regardless of success/failure
            end_time = datetime.now(timezone.utc)
            source_tables = [
                f"{asset.get('project_id')}.{asset.get('dataset_id')}.{asset.get('table_id')}"
                for asset in assets
                if asset.get('project_id') and asset.get('dataset_id') and asset.get('table_id')
            ]
            
            if source_tables:
                # Build source-target pairs for lineage
                target_fqn = format_bigquery_fqn(self.project_id, self.dataset_id, self.table_id)
                source_targets = [
                    (format_bigquery_fqn(*table.split('.')), target_fqn)
                    for table in source_tables
                    if '.' in table  # Ensure table is in project.dataset.table format
                ]
                
                record_lineage(
                    project_id=self.project_id,
                    location=self.lineage_location,
                    process_name=self.dag_name,
                    task_id=self.task_id,
                    source_targets=source_targets,
                    start_time=start_time,
                    end_time=end_time,
                    is_success=is_success,
                    source_system="bigquery",
                    source_type="metadata_extraction",
                    extraction_method="discovery"
                )
