from google.cloud import bigquery
from google.cloud.exceptions import NotFound
import logging
import os
from typing import List, Dict, Any
from datetime import datetime, timezone

logger = logging.getLogger(__name__)


class BigQueryWriter:
    def __init__(self, project_id: str, dataset_id: str = None, table_id: str = None):
        self.project_id = project_id
        self.client = bigquery.Client(project=project_id)
        self.dataset_id = dataset_id or os.getenv("BQ_DATASET", "data_discovery")
        self.table_id = table_id or os.getenv("BQ_TABLE", "discovered_assets")
        self.location = os.getenv("BQ_LOCATION", "US")
        self.run_timestamp = datetime.now(timezone.utc)

    def _get_bigquery_schema(self):
        return [
            bigquery.SchemaField("table_id", "STRING", "REQUIRED", description="The ID of the BigQuery table."),
            bigquery.SchemaField("project_id", "STRING", description="The GCP project ID containing the table."),
            bigquery.SchemaField("dataset_id", "STRING", description="The BigQuery dataset ID containing the table."),
            bigquery.SchemaField("description", "STRING", description="The user-provided or Gemini-generated description of the table."),
            bigquery.SchemaField("table_type", "STRING", description="The type of the table (e.g., TABLE, VIEW)."),
            bigquery.SchemaField("created", "TIMESTAMP", description="The timestamp when the table was created."),
            bigquery.SchemaField("last_modified", "TIMESTAMP", description="The timestamp when the table was last modified."),
            bigquery.SchemaField("last_accessed", "TIMESTAMP", description="The timestamp when the table was last accessed."),
            bigquery.SchemaField("row_count", "INTEGER", description="The total number of rows in the table."),
            bigquery.SchemaField("column_count", "INTEGER", description="The total number of columns in the table."),
            bigquery.SchemaField("size_bytes", "INTEGER", description="The total size of the table in bytes."),
            bigquery.SchemaField("has_pii", "BOOLEAN", description="Indicates if the table contains Personally Identifiable Information (PII)."),
            bigquery.SchemaField("has_phi", "BOOLEAN", description="Indicates if the table contains Protected Health Information (PHI)."),
            bigquery.SchemaField("environment", "STRING", description="The environment the table belongs to (e.g., PROD, DEV)."),
            bigquery.SchemaField("labels", "RECORD", "REPEATED", fields=[
                bigquery.SchemaField("key", "STRING", description="The key of a label attached to the table."),
                bigquery.SchemaField("value", "STRING", description="The value of a label attached to the table."),
            ], description="Key-value labels attached to the table."),
            bigquery.SchemaField("schema", "RECORD", "REPEATED", fields=[
                bigquery.SchemaField("name", "STRING", description="The name of the column."),
                bigquery.SchemaField("type", "STRING", description="The data type of the column."),
                bigquery.SchemaField("mode", "STRING", description="The mode of the column (e.g., NULLABLE, REQUIRED)."),
                bigquery.SchemaField("description", "STRING", description="The description of the column."),
                bigquery.SchemaField("sample_values", "STRING", "REPEATED", description="A list of sample values for the column."),
            ], description="The schema of the table, including all column definitions."),
            bigquery.SchemaField("analytical_insights", "STRING", "REPEATED", description="A list of analytical questions or insights that can be derived from the table, generated by Gemini."),
            bigquery.SchemaField("lineage", "RECORD", "REPEATED", fields=[
                bigquery.SchemaField("source", "STRING", description="The source table or resource."),
                bigquery.SchemaField("target", "STRING", description="The target table or resource."),
            ], description="Data lineage information, showing upstream sources and downstream targets."),
            bigquery.SchemaField("column_profiles", "RECORD", "REPEATED", fields=[
                bigquery.SchemaField("column_name", "STRING"),
                bigquery.SchemaField("profile_type", "STRING"),
                bigquery.SchemaField("min_value", "STRING"),
                bigquery.SchemaField("max_value", "STRING"),
                bigquery.SchemaField("avg_value", "STRING"),
                bigquery.SchemaField("distinct_count", "INTEGER"),
                bigquery.SchemaField("null_percentage", "FLOAT"),
            ], description="Statistical profiles for each column."),
            bigquery.SchemaField("key_metrics", "RECORD", "REPEATED", fields=[
                bigquery.SchemaField("metric_name", "STRING"),
                bigquery.SchemaField("metric_value", "STRING"),
            ], description="Key metrics associated with the table, such as completeness or freshness scores."),
            bigquery.SchemaField("run_timestamp", "TIMESTAMP", "REQUIRED", description="The UTC timestamp of the collection run, used to identify all assets from a single execution."),
            bigquery.SchemaField("insert_timestamp", "TIMESTAMP", "REQUIRED", description="The UTC timestamp when this metadata record was inserted."),
        ]

    def _ensure_dataset_exists(self):
        """Creates the BigQuery dataset if it does not already exist."""
        dataset_ref = self.client.dataset(self.dataset_id)
        try:
            self.client.get_dataset(dataset_ref)
            logger.info(f"Dataset {self.dataset_id} already exists.")
        except NotFound:
            logger.info(f"Dataset {self.dataset_id} not found, creating it.")
            dataset = bigquery.Dataset(dataset_ref)
            dataset.description = "Contains discovered data assets and metadata from the Data Discovery Agent."
            dataset.location = self.location
            self.client.create_dataset(dataset, exists_ok=True)
            logger.info(f"Dataset {self.dataset_id} created in location {dataset.location}.")

    def _create_or_update_latest_view(self):
        """Creates or replaces a view that points to the latest run."""
        view_id = f"{self.table_id}_latest"
        view_ref = f"{self.project_id}.{self.dataset_id}.{view_id}"

        view_query = f"""
        CREATE OR REPLACE VIEW `{view_ref}`
        OPTIONS(
            description="A view that shows the most recent metadata collected, based on the latest run timestamp."
        ) AS
        SELECT * EXCEPT(run_timestamp)
        FROM `{self.project_id}.{self.dataset_id}.{self.table_id}`
        WHERE run_timestamp = (
            SELECT MAX(run_timestamp)
            FROM `{self.project_id}.{self.dataset_id}.{self.table_id}`
        )
        """
        try:
            query_job = self.client.query(view_query)
            query_job.result()  # Wait for the job to complete
            logger.info(f"Successfully created or updated view: {view_ref}")
        except Exception as e:
            logger.error(f"Failed to create or update latest view {view_id}: {e}")
            # Don't fail the whole process if view creation fails
            pass

    def write_to_bigquery(self, assets: List[Dict[str, Any]]):
        self._ensure_dataset_exists()
        table_ref = self.client.dataset(self.dataset_id).table(self.table_id)

        schema = self._get_bigquery_schema()
        table = bigquery.Table(table_ref, schema=schema)
        table.description = "A centralized catalog of discovered BigQuery assets, including metadata, schema, lineage, and profiling information."
        table.time_partitioning = bigquery.TimePartitioning(
            type_=bigquery.TimePartitioningType.DAY,
            field="insert_timestamp"
        )
        table = self.client.create_table(table, exists_ok=True)
        logger.info(f"Ensured BigQuery table {table.project}.{table.dataset_id}.{table.table_id} exists.")

        rows_to_insert = []
        for asset in assets:
            struct_data = asset.get("struct_data", {})
            quality_stats = struct_data.get("quality_stats", {}) or {}
            schema_info = struct_data.get("schema_info", {}) or {}
            
            # Prepare schema with sample values
            schema_fields = []
            if schema_info and "fields" in schema_info:
                sample_values = quality_stats.get("sample_values", {})
                for field in schema_info["fields"]:
                    field_name = field.get("name")
                    schema_fields.append({
                        "name": field_name,
                        "type": field.get("type"),
                        "mode": field.get("mode"),
                        "description": field.get("description"),
                        "sample_values": sample_values.get(field_name, [])
                    })

            # Prepare column profiles
            column_profiles_raw = struct_data.get("column_profiles", {})
            column_profiles = []
            if column_profiles_raw:
                for col_name, profile in column_profiles_raw.items():
                    column_profiles.append({
                        "column_name": col_name,
                        "profile_type": profile.get("type"),
                        "min_value": str(profile.get("min", "")),
                        "max_value": str(profile.get("max", "")),
                        "avg_value": str(profile.get("avg", "")),
                        "distinct_count": profile.get("distinct_count"),
                        "null_percentage": quality_stats.get("columns", {}).get(col_name, {}).get("null_percentage")
                    })
            
            # Prepare key metrics
            key_metrics = []
            if completeness := quality_stats.get("completeness_score"):
                key_metrics.append({"metric_name": "completeness_score", "metric_value": str(completeness)})
            if freshness := quality_stats.get("freshness_score"):
                key_metrics.append({"metric_name": "freshness_score", "metric_value": str(freshness)})
            if volatility := struct_data.get("volatility"):
                key_metrics.append({"metric_name": "volatility", "metric_value": volatility})
            if cache_ttl := struct_data.get("cache_ttl"):
                key_metrics.append({"metric_name": "cache_ttl", "metric_value": cache_ttl})

            # Prepare lineage
            lineage = []
            lineage_raw = struct_data.get("lineage")
            table_full_name = f"{struct_data.get('project_id')}.{struct_data.get('dataset_id')}.{struct_data.get('table_id')}"
            if lineage_raw:
                for upstream_table in lineage_raw.get("upstream_tables", []):
                    lineage.append({"source": upstream_table, "target": table_full_name})
                for downstream_table in lineage_raw.get("downstream_tables", []):
                    lineage.append({"source": table_full_name, "target": downstream_table})


            rows_to_insert.append({
                "table_id": struct_data.get("table_id"),
                "project_id": struct_data.get("project_id"),
                "dataset_id": struct_data.get("dataset_id"),
                "description": struct_data.get("description"),
                "table_type": struct_data.get("table_type"),
                "created": struct_data.get("created_timestamp"),
                "last_modified": struct_data.get("last_modified_timestamp"),
                "last_accessed": struct_data.get("last_accessed_timestamp"),
                "row_count": struct_data.get("row_count"),
                "column_count": struct_data.get("column_count"),
                "size_bytes": struct_data.get("size_bytes"),
                "has_pii": struct_data.get("has_pii"),
                "has_phi": struct_data.get("has_phi"),
                "environment": struct_data.get("environment"),
                "labels": [{"key": k, "value": v} for k, v in struct_data.get("labels", {}).items()],
                "schema": schema_fields,
                "analytical_insights": quality_stats.get("insights", []),
                "lineage": lineage,
                "column_profiles": column_profiles,
                "key_metrics": key_metrics,
                "run_timestamp": self.run_timestamp.isoformat(),
                "insert_timestamp": "AUTO"
            })
            
        if not rows_to_insert:
            logger.info("No rows to insert into BigQuery.")
            return

        errors = self.client.insert_rows_json(table, rows_to_insert)
        if errors:
            logger.error(f"Errors inserting rows into BigQuery: {errors}")
            raise Exception(f"BigQuery insert failed: {errors}")
        else:
            logger.info(f"Successfully inserted {len(rows_to_insert)} rows into {table_ref.path}")

        self._create_or_update_latest_view()
