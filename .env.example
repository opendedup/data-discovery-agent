# =============================================================================
# Data Discovery Agent - Environment Variables Example
# =============================================================================
# Copy this file to .env and fill in your actual values
# NEVER commit the .env file to git - it contains sensitive information!
# =============================================================================

# -----------------------------------------------------------------------------
# Google Cloud Configuration
# -----------------------------------------------------------------------------

# Your Google Cloud Project ID
GCP_PROJECT_ID=your-project-id-heres

# Google Cloud Storage bucket for JSONL data files
GCS_JSONL_BUCKET=your-jsonl-bucket-name

# Google Cloud Storage bucket for generated reports
GCS_REPORTS_BUCKET=your-reports-bucket-name

# Vertex AI Search datastore ID
VERTEX_DATASTORE_ID=your-datastore-id

# Vertex AI location (e.g., "global", "us-central1", "europe-west1")
VERTEX_LOCATION=global

# -----------------------------------------------------------------------------
# BigQuery Export Configuration
# -----------------------------------------------------------------------------
# The dataset and table to export discovered metadata to.
# The writer will create the dataset and table if they don't exist.
# 
# Note: The MCP discovery tools also query the {BQ_TABLE}_latest view 
# to enrich search results with complete schema, lineage, analytical_insights,
# and column_profiles data.
BQ_DATASET=data_discovery
BQ_TABLE=discovered_assets
BQ_LOCATION=US

# -----------------------------------------------------------------------------
# API Keys
# -----------------------------------------------------------------------------

# Gemini API Key for AI/ML features
GEMINI_API_KEY=your-gemini-api-key-here

# -----------------------------------------------------------------------------
# Lineage Tracking Configuration
# -----------------------------------------------------------------------------

# Enable or disable lineage tracking (true/false)
LINEAGE_ENABLED=true

# GCP region for lineage API (should match your main region)
LINEAGE_LOCATION=us-central1

# -----------------------------------------------------------------------------
# MCP Service Configuration
# -----------------------------------------------------------------------------

# MCP server name
MCP_SERVER_NAME=data-discovery-agent

# MCP server version
MCP_SERVER_VERSION=1.0.0

# MCP transport mode (stdio or http)
# - stdio: For local development and subprocess communication (default)
# - http: For containerized deployment and remote connections
MCP_TRANSPORT=stdio

# Host address for HTTP server (only used when MCP_TRANSPORT=http)
# Use 0.0.0.0 in containers to accept connections from any interface
MCP_HOST=0.0.0.0

# Port for MCP HTTP service (only used when MCP_TRANSPORT=http)
MCP_PORT=8080

# Default number of results per page
MCP_DEFAULT_PAGE_SIZE=10

# Maximum number of results per page
MCP_MAX_PAGE_SIZE=50

# Query timeout in seconds
MCP_QUERY_TIMEOUT=30.0

# Include full markdown content in responses (true/false)
MCP_INCLUDE_FULL_MARKDOWN=true

# Enable BigQuery console links in responses (true/false)
MCP_ENABLE_CONSOLE_LINKS=true

# -----------------------------------------------------------------------------
# PRP Discovery Configuration
# -----------------------------------------------------------------------------
# Configuration for Product Requirement Prompt (PRP) analysis and dataset discovery

# Maximum number of search queries to generate from a PRP
# Higher values provide more comprehensive search but may be slower
PRP_MAX_QUERIES=10

# Minimum relevance score for dataset inclusion (0-100)
# Datasets scoring below this threshold will be filtered out
PRP_MIN_RELEVANCE_SCORE=60.0

# -----------------------------------------------------------------------------
# Airflow Scratch Storage Configuration
# -----------------------------------------------------------------------------

# Scratch file retention for debugging (days)
# Airflow DAG tasks write intermediate data to GCS scratch storage.
# This setting controls how long to keep these files before automatic cleanup.
SCRATCH_RETENTION_DAYS=7

# -----------------------------------------------------------------------------
# Optional Configuration
# -----------------------------------------------------------------------------

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
# LOG_LEVEL=INFO
